{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hospital Readmission Prediction - Modeling\n",
    "\n",
    "This notebook performs model training on the Diabetes 130-US Hospitals dataset to understand patterns related to patient readmissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    precision_recall_curve, roc_curve, average_precision_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# File paths\n",
    "DATA_DIR = Path(\"./data\")\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "RESULTS_DIR = Path(\"./reports/results\")\n",
    "FIGURES_DIR = Path(\"./reports/figures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories():\n",
    "    \"\"\"Create necessary directories if they don't exist\"\"\"\n",
    "    for dir_path in [MODELS_DIR, RESULTS_DIR, FIGURES_DIR]:\n",
    "        if not dir_path.exists():\n",
    "            dir_path.mkdir(parents=True)\n",
    "            logger.info(f\"Created directory: {dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data():\n",
    "    \"\"\"Load the preprocessed data for modeling\"\"\"\n",
    "    try:\n",
    "        data_path = PROCESSED_DATA_DIR / \"model_ready_data.pkl\"\n",
    "        processed_data = joblib.load(data_path)\n",
    "        logger.info(f\"Loaded processed data from {data_path}\")\n",
    "        \n",
    "        X_train = processed_data['X_train']\n",
    "        X_test = processed_data['X_test']\n",
    "        y_train = processed_data['y_train']\n",
    "        y_test = processed_data['y_test']\n",
    "        \n",
    "        # Check for string values in the data\n",
    "        if isinstance(X_train, np.ndarray):\n",
    "            # Check if any element is a string\n",
    "            if X_train.dtype == object:\n",
    "                logger.warning(\"Found object dtype in X_train, converting strings to numeric...\")\n",
    "                # Try to convert strings to numeric (one-hot encode any remaining strings)\n",
    "                from sklearn.preprocessing import OneHotEncoder\n",
    "                import pandas as pd\n",
    "                \n",
    "                # Convert to DataFrame for easier handling\n",
    "                X_train_df = pd.DataFrame(X_train)\n",
    "                X_test_df = pd.DataFrame(X_test)\n",
    "                \n",
    "                # Find columns with string values\n",
    "                string_cols = []\n",
    "                for col in X_train_df.columns:\n",
    "                    if X_train_df[col].dtype == object:\n",
    "                        try:\n",
    "                            # Try to convert to numeric\n",
    "                            X_train_df[col] = pd.to_numeric(X_train_df[col], errors='raise')\n",
    "                            X_test_df[col] = pd.to_numeric(X_test_df[col], errors='raise')\n",
    "                        except:\n",
    "                            string_cols.append(col)\n",
    "                \n",
    "                if string_cols:\n",
    "                    logger.warning(f\"Found string columns: {string_cols}\")\n",
    "                    # One-hot encode string columns\n",
    "                    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "                    \n",
    "                    # Get string column data\n",
    "                    X_train_str = X_train_df[string_cols].values\n",
    "                    X_test_str = X_test_df[string_cols].values\n",
    "                    \n",
    "                    # Fit and transform\n",
    "                    X_train_str_encoded = ohe.fit_transform(X_train_str)\n",
    "                    X_test_str_encoded = ohe.transform(X_test_str)\n",
    "                    \n",
    "                    # Get non-string columns\n",
    "                    non_str_cols = [c for c in X_train_df.columns if c not in string_cols]\n",
    "                    X_train_non_str = X_train_df[non_str_cols].values\n",
    "                    X_test_non_str = X_test_df[non_str_cols].values\n",
    "                    \n",
    "                    # Combine encoded and non-encoded parts\n",
    "                    X_train = np.hstack([X_train_non_str, X_train_str_encoded])\n",
    "                    X_test = np.hstack([X_test_non_str, X_test_str_encoded])\n",
    "                    \n",
    "                    logger.info(f\"Successfully encoded string columns, new shape: {X_train.shape}\")\n",
    "                else:\n",
    "                    # If no string columns were found, convert back to numpy array\n",
    "                    X_train = X_train_df.values\n",
    "                    X_test = X_test_df.values\n",
    "        \n",
    "        # Update processed data with fixed arrays\n",
    "        processed_data['X_train'] = X_train\n",
    "        processed_data['X_test'] = X_test\n",
    "        \n",
    "        logger.info(f\"Loaded data with {X_train.shape[0]} training samples and {X_test.shape[0]} test samples\")\n",
    "        logger.info(f\"Class distribution in training set: {np.bincount(y_train)}\")\n",
    "        logger.info(f\"Class distribution in test set: {np.bincount(y_test)}\")\n",
    "        \n",
    "        return processed_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load processed data: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train, class_weight='balanced'):\n",
    "    \"\"\"Train a logistic regression model\"\"\"\n",
    "    try:\n",
    "        # Create and train the model\n",
    "        lr_model = LogisticRegression(\n",
    "            C=1.0,\n",
    "            class_weight=class_weight,\n",
    "            solver='liblinear',\n",
    "            random_state=42,\n",
    "            max_iter=1000\n",
    "        )\n",
    "        \n",
    "        lr_model.fit(X_train, y_train)\n",
    "        logger.info(\"Logistic Regression model trained successfully\")\n",
    "        \n",
    "        return lr_model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to train Logistic Regression model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train, class_weight='balanced'):\n",
    "    \"\"\"Train a random forest model\"\"\"\n",
    "    try:\n",
    "        # Create and train the model\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=4,\n",
    "            class_weight=class_weight,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rf_model.fit(X_train, y_train)\n",
    "        logger.info(\"Random Forest model trained successfully\")\n",
    "        \n",
    "        return rf_model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to train Random Forest model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_boosting(X_train, y_train):\n",
    "    \"\"\"Train a gradient boosting model\"\"\"\n",
    "    try:\n",
    "        # Create and train the model\n",
    "        gb_model = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        gb_model.fit(X_train, y_train)\n",
    "        logger.info(\"Gradient Boosting model trained successfully\")\n",
    "        \n",
    "        return gb_model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to train Gradient Boosting model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X_train, y_train, scale_pos_weight=None):\n",
    "    \"\"\"Train an XGBoost model\"\"\"\n",
    "    try:\n",
    "        # Create and train the model\n",
    "        if scale_pos_weight is None:\n",
    "            # Calculate class weight based on class distribution\n",
    "            neg_count = np.sum(y_train == 0)\n",
    "            pos_count = np.sum(y_train == 1)\n",
    "            scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            min_child_weight=2,\n",
    "            gamma=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective='binary:logistic',\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        \n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        logger.info(\"XGBoost model trained successfully\")\n",
    "        \n",
    "        return xgb_model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to train XGBoost model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm(X_train, y_train, class_weight=None):\n",
    "    \"\"\"Train a LightGBM model\"\"\"\n",
    "    try:\n",
    "        # Create and train the model\n",
    "        if class_weight is None:\n",
    "            # Calculate class weight based on class distribution\n",
    "            neg_count = np.sum(y_train == 0)\n",
    "            pos_count = np.sum(y_train == 1)\n",
    "            class_weight = {0: 1.0, 1: neg_count / pos_count if pos_count > 0 else 1.0}\n",
    "        \n",
    "        lgb_model = lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            num_leaves=31,\n",
    "            min_child_samples=20,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            class_weight=class_weight,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        logger.info(\"LightGBM model trained successfully\")\n",
    "        \n",
    "        return lgb_model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to train LightGBM model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_smote(model, X_train, y_train, random_state=42):\n",
    "    \"\"\"Train a model with SMOTE oversampling for imbalanced data\"\"\"\n",
    "    try:\n",
    "        # Create a pipeline with SMOTE and the classifier\n",
    "        smote_pipeline = ImbPipeline([\n",
    "            ('smote', SMOTE(random_state=random_state)),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        \n",
    "        # Fit the pipeline\n",
    "        smote_pipeline.fit(X_train, y_train)\n",
    "        logger.info(f\"Model with SMOTE trained successfully: {type(model).__name__}\")\n",
    "        \n",
    "        return smote_pipeline\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to train model with SMOTE: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, threshold=0.5):\n",
    "    \"\"\"Evaluate a trained model and return metrics\"\"\"\n",
    "    try:\n",
    "        # Get predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Get probability predictions for AUC and PR curves if possible\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        elif hasattr(model, 'named_steps') and hasattr(model.named_steps['classifier'], 'predict_proba'):\n",
    "            y_prob = model.named_steps['classifier'].predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_prob = y_pred  # Fall back to binary predictions if probabilities not available\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        # AUC and average precision\n",
    "        if len(np.unique(y_test)) > 1:  # Only calculate if both classes present\n",
    "            roc_auc = roc_auc_score(y_test, y_prob)\n",
    "            avg_precision = average_precision_score(y_test, y_prob)\n",
    "        else:\n",
    "            roc_auc = np.nan\n",
    "            avg_precision = np.nan\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Generate classification report\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        # Collect results\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'avg_precision': avg_precision,\n",
    "            'confusion_matrix': cm,\n",
    "            'classification_report': report,\n",
    "            'y_true': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'y_prob': y_prob\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Evaluation results for {model_name}:\")\n",
    "        logger.info(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        logger.info(f\"  Precision: {precision:.4f}\")\n",
    "        logger.info(f\"  Recall: {recall:.4f}\")\n",
    "        logger.info(f\"  F1 Score: {f1:.4f}\")\n",
    "        logger.info(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to evaluate model {model_name}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(results_dict, save_path=None):\n",
    "    \"\"\"Plot Precision-Recall curves for all models\"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for model_name, results in results_dict.items():\n",
    "            y_true = results['y_true']\n",
    "            y_prob = results['y_prob']\n",
    "            \n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "            avg_precision = results['avg_precision']\n",
    "            \n",
    "            plt.plot(recall, precision, lw=2, label=f'{model_name} (AP = {avg_precision:.3f})')\n",
    "        \n",
    "        # Calculate the no-skill baseline (prevalence)\n",
    "        prevalence = np.sum(y_true) / len(y_true) if len(y_true) > 0 else 0\n",
    "        plt.plot([0, 1], [prevalence, prevalence], 'k--', lw=2, label=f'No Skill (AP = {prevalence:.3f})')\n",
    "        \n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curves')\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"Precision-Recall curve plot saved to {save_path}\")\n",
    "        \n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to plot Precision-Recall curves: {e}\")\n",
    "        raise\n",
    "\n",
    "def plot_confusion_matrices(results_dict, save_path=None):\n",
    "    \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "    try:\n",
    "        n_models = len(results_dict)\n",
    "        fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 5))\n",
    "        \n",
    "        if n_models == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, (model_name, results) in enumerate(results_dict.items()):\n",
    "            cm = results['confusion_matrix']\n",
    "            \n",
    "            # Create a prettier confusion matrix with percentages\n",
    "            cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            cm_norm = np.nan_to_num(cm_norm)  # Replace NaN with 0\n",
    "            \n",
    "            # Plot the confusion matrix\n",
    "            sns.heatmap(\n",
    "                cm, \n",
    "                annot=True, \n",
    "                fmt=\"d\", \n",
    "                cmap=\"Blues\", \n",
    "                ax=axes[i],\n",
    "                cbar=False\n",
    "            )\n",
    "            \n",
    "            # Add percentage labels\n",
    "            for j in range(cm.shape[0]):\n",
    "                for k in range(cm.shape[1]):\n",
    "                    axes[i].text(\n",
    "                        k + 0.5, \n",
    "                        j + 0.5, \n",
    "                        f\"{cm_norm[j, k]:.1%}\",\n",
    "                        ha=\"center\", \n",
    "                        va=\"center\", \n",
    "                        fontsize=9,\n",
    "                        color=\"white\" if cm_norm[j, k] > 0.5 else \"black\"\n",
    "                    )\n",
    "            \n",
    "            axes[i].set_title(f\"{model_name}\")\n",
    "            axes[i].set_xlabel(\"Predicted label\")\n",
    "            axes[i].set_ylabel(\"True label\")\n",
    "            axes[i].set_xticklabels(['Not Readmitted', 'Readmitted'])\n",
    "            axes[i].set_yticklabels(['Not Readmitted', 'Readmitted'])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"Confusion matrices plot saved to {save_path}\")\n",
    "        \n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to plot confusion matrices: {e}\")\n",
    "        raise\n",
    "\n",
    "def plot_feature_importance(model, feature_names, model_name, top_n=20, save_path=None):\n",
    "    \"\"\"Plot feature importance for the model\"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            # For tree-based models that have feature_importances_ attribute\n",
    "            importances = model.feature_importances_\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            # For linear models like logistic regression\n",
    "            importances = np.abs(model.coef_[0])\n",
    "        elif hasattr(model, 'named_steps') and hasattr(model.named_steps.get('classifier', None), 'feature_importances_'):\n",
    "            # For pipeline with classifier that has feature_importances_\n",
    "            importances = model.named_steps['classifier'].feature_importances_\n",
    "        elif hasattr(model, 'named_steps') and hasattr(model.named_steps.get('classifier', None), 'coef_'):\n",
    "            # For pipeline with classifier that has coef_\n",
    "            importances = np.abs(model.named_steps['classifier'].coef_[0])\n",
    "        else:\n",
    "            logger.warning(f\"Model {model_name} does not have feature importances or coefficients\")\n",
    "            return\n",
    "        \n",
    "        # Get feature importance and names\n",
    "        if len(importances) > len(feature_names):\n",
    "            logger.warning(f\"Mismatch between importances length {len(importances)} and feature names length {len(feature_names)}\")\n",
    "            # Truncate importances to match feature_names\n",
    "            importances = importances[:len(feature_names)]\n",
    "        elif len(importances) < len(feature_names):\n",
    "            logger.warning(f\"Mismatch between importances length {len(importances)} and feature names length {len(feature_names)}\")\n",
    "            # Truncate feature_names to match importances\n",
    "            feature_names = feature_names[:len(importances)]\n",
    "        \n",
    "        # Create DataFrame for sorting\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        })\n",
    "        \n",
    "        # Sort by importance and get top N\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=False).head(top_n)\n",
    "        \n",
    "        # Plot horizontal bar chart\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "        plt.title(f\"Top {top_n} Feature Importances - {model_name}\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"Feature importance plot saved to {save_path}\")\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "        return importance_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to plot feature importance for {model_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "def plot_shap_summary(model, X_test, feature_names, model_name, max_display=20, save_path=None):\n",
    "    \"\"\"Plot SHAP summary for the model\"\"\"\n",
    "    try:\n",
    "        # Create a figure\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # For tree-based models\n",
    "        if isinstance(model, (RandomForestClassifier, GradientBoostingClassifier)) or \\\n",
    "           isinstance(model, xgb.XGBClassifier) or \\\n",
    "           isinstance(model, lgb.LGBMClassifier):\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            \n",
    "            # Limit to a sample for computational efficiency\n",
    "            sample_size = min(500, X_test.shape[0])\n",
    "            X_sample = X_test[:sample_size]\n",
    "            \n",
    "            shap_values = explainer.shap_values(X_sample)\n",
    "            \n",
    "            # Handle different formats returned by different models\n",
    "            if isinstance(shap_values, list):\n",
    "                # For models that return a list of shap values for each class\n",
    "                class_to_show = 1  # Show SHAP values for the positive class (readmission)\n",
    "                shap_vals = shap_values[class_to_show] if len(shap_values) > 1 else shap_values[0]\n",
    "            else:\n",
    "                shap_vals = shap_values\n",
    "            \n",
    "            # Use feature names for the plot\n",
    "            feature_names_truncated = [name[:30] + '...' if len(name) > 30 else name for name in feature_names]\n",
    "            \n",
    "            if len(feature_names_truncated) > X_sample.shape[1]:\n",
    "                # Truncate feature names if needed\n",
    "                feature_names_truncated = feature_names_truncated[:X_sample.shape[1]]\n",
    "            elif len(feature_names_truncated) < X_sample.shape[1]:\n",
    "                # Add generic names if needed\n",
    "                feature_names_truncated += [f\"Feature_{i}\" for i in range(len(feature_names_truncated), X_sample.shape[1])]\n",
    "            \n",
    "            # Create SHAP summary plot\n",
    "            shap.summary_plot(\n",
    "                shap_vals, \n",
    "                X_sample, \n",
    "                feature_names=feature_names_truncated,\n",
    "                max_display=max_display,\n",
    "                show=False\n",
    "            )\n",
    "            \n",
    "        # For linear models\n",
    "        elif isinstance(model, LogisticRegression):\n",
    "            explainer = shap.LinearExplainer(model, X_test)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            \n",
    "            # Use feature names for the plot\n",
    "            feature_names_truncated = [name[:30] + '...' if len(name) > 30 else name for name in feature_names]\n",
    "            \n",
    "            # Create SHAP summary plot\n",
    "            shap.summary_plot(\n",
    "                shap_values, \n",
    "                X_test, \n",
    "                feature_names=feature_names_truncated, \n",
    "                max_display=max_display,\n",
    "                show=False\n",
    "            )\n",
    "        \n",
    "        # For pipeline models, extract the classifier\n",
    "        elif hasattr(model, 'named_steps') and 'classifier' in model.named_steps:\n",
    "            classifier = model.named_steps['classifier']\n",
    "            \n",
    "            if isinstance(classifier, (RandomForestClassifier, GradientBoostingClassifier)) or \\\n",
    "               isinstance(classifier, xgb.XGBClassifier) or \\\n",
    "               isinstance(classifier, lgb.LGBMClassifier):\n",
    "                explainer = shap.TreeExplainer(classifier)\n",
    "            elif isinstance(classifier, LogisticRegression):\n",
    "                explainer = shap.LinearExplainer(classifier, X_test)\n",
    "            else:\n",
    "                logger.warning(f\"Unsupported classifier type for SHAP: {type(classifier)}\")\n",
    "                return\n",
    "            \n",
    "            # Limit to a sample for computational efficiency\n",
    "            sample_size = min(500, X_test.shape[0])\n",
    "            X_sample = X_test[:sample_size]\n",
    "            \n",
    "            shap_values = explainer.shap_values(X_sample)\n",
    "            \n",
    "            # Handle different formats returned by different models\n",
    "            if isinstance(shap_values, list):\n",
    "                # For models that return a list of shap values for each class\n",
    "                class_to_show = 1  # Show SHAP values for the positive class (readmission)\n",
    "                shap_vals = shap_values[class_to_show] if len(shap_values) > 1 else shap_values[0]\n",
    "            else:\n",
    "                shap_vals = shap_values\n",
    "            \n",
    "            # Use feature names for the plot\n",
    "            feature_names_truncated = [name[:30] + '...' if len(name) > 30 else name for name in feature_names]\n",
    "            \n",
    "            # Create SHAP summary plot\n",
    "            shap.summary_plot(\n",
    "                shap_vals, \n",
    "                X_sample, \n",
    "                feature_names=feature_names_truncated, \n",
    "                max_display=max_display,\n",
    "                show=False\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            logger.warning(f\"Unsupported model type for SHAP: {type(model)}\")\n",
    "            return\n",
    "        \n",
    "        plt.title(f\"SHAP Summary - {model_name}\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"SHAP summary plot saved to {save_path}\")\n",
    "        \n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to plot SHAP summary for {model_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_model_comparison_table(results_dict, save_path=None):\n",
    "    \"\"\"Create a table comparing model performance metrics\"\"\"\n",
    "    try:\n",
    "        # Extract metrics from results\n",
    "        model_names = []\n",
    "        metrics = {\n",
    "            'Accuracy': [],\n",
    "            'Precision': [],\n",
    "            'Recall': [],\n",
    "            'F1 Score': [],\n",
    "            'ROC AUC': [],\n",
    "            'Avg Precision': []\n",
    "        }\n",
    "        \n",
    "        for model_name, results in results_dict.items():\n",
    "            model_names.append(model_name)\n",
    "            metrics['Accuracy'].append(results['accuracy'])\n",
    "            metrics['Precision'].append(results['precision'])\n",
    "            metrics['Recall'].append(results['recall'])\n",
    "            metrics['F1 Score'].append(results['f1_score'])\n",
    "            metrics['ROC AUC'].append(results['roc_auc'])\n",
    "            metrics['Avg Precision'].append(results['avg_precision'])\n",
    "        \n",
    "        # Create DataFrame\n",
    "        comparison_df = pd.DataFrame(metrics, index=model_names)\n",
    "        \n",
    "        # Format as percentages\n",
    "        comparison_df = comparison_df.apply(lambda x: x.map('{:.2%}'.format))\n",
    "        \n",
    "        # Save to CSV\n",
    "        if save_path:\n",
    "            comparison_df.to_csv(save_path)\n",
    "            logger.info(f\"Model comparison table saved to {save_path}\")\n",
    "        \n",
    "        return comparison_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create model comparison table: {e}\")\n",
    "        raise\n",
    "\n",
    "def save_model(model, model_name):\n",
    "    \"\"\"Save the trained model to disk\"\"\"\n",
    "    try:\n",
    "        # Create model file path\n",
    "        model_path = MODELS_DIR / f\"{model_name}.pkl\"\n",
    "        \n",
    "        # Save the model\n",
    "        joblib.dump(model, model_path)\n",
    "        logger.info(f\"Model saved to {model_path}\")\n",
    "        \n",
    "        return model_path\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save model {model_name}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traning the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "create_directories()\n",
    "\n",
    "# Load processed data\n",
    "processed_data = load_processed_data()\n",
    "X_train = processed_data['X_train']\n",
    "X_test = processed_data['X_test']\n",
    "y_train = processed_data['y_train']\n",
    "y_test = processed_data['y_test']\n",
    "feature_names = processed_data['feature_names']\n",
    "\n",
    "logger.info(f\"Loaded data with {X_train.shape[0]} training samples and {X_test.shape[0]} test samples\")\n",
    "logger.info(f\"Class distribution in training set: {np.bincount(y_train)}\")\n",
    "logger.info(f\"Class distribution in test set: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "models = {\n",
    "    'Logistic Regression': train_logistic_regression(X_train, y_train),\n",
    "    'Random Forest': train_random_forest(X_train, y_train),\n",
    "    'Gradient Boosting': train_gradient_boosting(X_train, y_train),\n",
    "    'XGBoost': train_xgboost(X_train, y_train),\n",
    "    'LightGBM': train_lightgbm(X_train, y_train)\n",
    "}\n",
    "\n",
    "# Also train a model with SMOTE to handle class imbalance\n",
    "models['XGBoost with SMOTE'] = train_with_smote(\n",
    "    xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='binary:logistic',\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    ),\n",
    "    X_train, y_train\n",
    ")\n",
    "\n",
    "# Evaluate models\n",
    "results_dict = {}\n",
    "for model_name, model in models.items():\n",
    "    results = evaluate_model(model, X_test, y_test, model_name)\n",
    "    results_dict[model_name] = results\n",
    "\n",
    "    # Save the model\n",
    "    save_model(model, model_name.replace(' ', '_').lower())\n",
    "\n",
    "# Create comparison plots and tables\n",
    "plot_roc_curve(results_dict, save_path=FIGURES_DIR / \"roc_curves.png\")\n",
    "plot_precision_recall_curve(results_dict, save_path=FIGURES_DIR / \"pr_curves.png\")\n",
    "plot_confusion_matrices(results_dict, save_path=FIGURES_DIR / \"confusion_matrices.png\")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = create_model_comparison_table(\n",
    "    results_dict, \n",
    "    save_path=RESULTS_DIR / \"model_comparison.csv\"\n",
    ")\n",
    "\n",
    "# Plot feature importance for the best model\n",
    "# Find the best model based on F1 score\n",
    "best_model_name = max(results_dict.items(), key=lambda x: x[1]['f1_score'])[0]\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "logger.info(f\"Best model based on F1 score: {best_model_name}\")\n",
    "\n",
    "# Check if the best model is a pipeline with SMOTE\n",
    "if hasattr(best_model, 'named_steps') and 'classifier' in best_model.named_steps:\n",
    "    plot_feature_importance(\n",
    "        best_model.named_steps['classifier'], \n",
    "        feature_names, \n",
    "        best_model_name,\n",
    "        save_path=FIGURES_DIR / \"feature_importance.png\"\n",
    "    )\n",
    "\n",
    "    # Plot SHAP values for the best model\n",
    "    plot_shap_summary(\n",
    "        best_model.named_steps['classifier'], \n",
    "        X_test, \n",
    "        feature_names, \n",
    "        best_model_name,\n",
    "        save_path=FIGURES_DIR / \"shap_summary.png\"\n",
    "    )\n",
    "else:\n",
    "    plot_feature_importance(\n",
    "        best_model, \n",
    "        feature_names, \n",
    "        best_model_name,\n",
    "        save_path=FIGURES_DIR / \"feature_importance.png\"\n",
    "    )\n",
    "\n",
    "    # Plot SHAP values for the best model\n",
    "    plot_shap_summary(\n",
    "        best_model, \n",
    "        X_test, \n",
    "        feature_names, \n",
    "        best_model_name,\n",
    "        save_path=FIGURES_DIR / \"shap_summary.png\"\n",
    "    )\n",
    "\n",
    "logger.info(\"Model training and evaluation complete\")\n",
    "\n",
    "return models, results_dict, comparison_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
