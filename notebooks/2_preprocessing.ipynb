{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hospital Readmission Prediction - Preprocessing\n",
    "\n",
    "This notebook performs preprocessing on the Diabetes 130-US Hospitals dataset to understand patterns related to patient readmissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import re\n",
    "import sklearn\n",
    "from packaging import version\n",
    "\n",
    "# File paths\n",
    "DATA_DIR = Path(\"./data\")\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "RAW_DATA_PATH = PROCESSED_DATA_DIR / \"diabetes_hospital_data.csv\"\n",
    "MAPPINGS_PATH = PROCESSED_DATA_DIR / \"IDs_mapping.csv\"\n",
    "PROCESSED_DATA_PATH = PROCESSED_DATA_DIR / \"diabetes_processed.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data():\n",
    "    \"\"\"Load the raw diabetes dataset\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(RAW_DATA_PATH)\n",
    "        print(f\"Raw data loaded with shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load raw data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mappings():\n",
    "    \"\"\"Load the ID mappings\"\"\"\n",
    "    try:\n",
    "        mappings = pd.read_csv(MAPPINGS_PATH)\n",
    "        print(f\"ID mappings loaded with shape: {mappings.shape}\")\n",
    "        \n",
    "        # Convert to usable dictionaries\n",
    "        mapping_dict = {}\n",
    "        \n",
    "        # Check if 'category' column exists\n",
    "        if 'category' not in mappings.columns:\n",
    "            # Create simpler mappings without categories\n",
    "            if 'id' in mappings.columns and 'description' in mappings.columns:\n",
    "                # For admission_type\n",
    "                admission_rows = mappings[mappings['description'].str.contains('admission type', case=False, na=False)]\n",
    "                if not admission_rows.empty:\n",
    "                    mapping_dict['admission_type'] = dict(zip(admission_rows['id'], admission_rows['description']))\n",
    "                \n",
    "                # For discharge_disposition\n",
    "                discharge_rows = mappings[mappings['description'].str.contains('discharge', case=False, na=False)]\n",
    "                if not discharge_rows.empty:\n",
    "                    mapping_dict['discharge_disposition'] = dict(zip(discharge_rows['id'], discharge_rows['description']))\n",
    "                \n",
    "                # For admission_source\n",
    "                source_rows = mappings[mappings['description'].str.contains('admission source', case=False, na=False)]\n",
    "                if not source_rows.empty:\n",
    "                    mapping_dict['admission_source'] = dict(zip(source_rows['id'], source_rows['description']))\n",
    "            else:\n",
    "                # Create dummy mapping dictionaries\n",
    "                print(\"Creating default mapping dictionaries as 'category' column is missing\")\n",
    "                mapping_dict['admission_type'] = {1: \"Emergency\", 2: \"Urgent\", 3: \"Elective\", 4: \"Newborn\"}\n",
    "                mapping_dict['discharge_disposition'] = {1: \"Discharged to home\", 2: \"Transferred to short term hospital\", 11: \"Expired\"}\n",
    "                mapping_dict['admission_source'] = {1: \"Physician Referral\", 7: \"Emergency Room\", 2: \"Clinic Referral\"}\n",
    "        else:\n",
    "            # Original code path when 'category' exists\n",
    "            for category in mappings['category'].unique():\n",
    "                category_df = mappings[mappings['category'] == category]\n",
    "                mapping_dict[category] = dict(zip(category_df['id'], category_df['description']))\n",
    "        \n",
    "        return mapping_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load ID mappings: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, mappings):\n",
    "    \"\"\"\n",
    "    Clean the dataset by:\n",
    "    - Handling missing values\n",
    "    - Decoding ID fields\n",
    "    - Recoding categorical variables\n",
    "    - Fixing data types\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Replace '?' with NaN\n",
    "        df_clean.replace('?', np.nan, inplace=True)\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_values = df_clean.isnull().sum()\n",
    "        print(f\"Missing values before imputation:\\n{missing_values[missing_values > 0]}\")\n",
    "        \n",
    "        # Map ID fields to their descriptions\n",
    "        if 'admission_type_id' in df_clean.columns and 'admission_type' in mappings:\n",
    "            df_clean['admission_type'] = df_clean['admission_type_id'].map(mappings['admission_type'])\n",
    "            \n",
    "        if 'discharge_disposition_id' in df_clean.columns and 'discharge_disposition' in mappings:\n",
    "            df_clean['discharge_disposition'] = df_clean['discharge_disposition_id'].map(mappings['discharge_disposition'])\n",
    "            \n",
    "        if 'admission_source_id' in df_clean.columns and 'admission_source' in mappings:\n",
    "            df_clean['admission_source'] = df_clean['admission_source_id'].map(mappings['admission_source'])\n",
    "        \n",
    "        # Convert numeric columns to appropriate types\n",
    "        numeric_cols = [\n",
    "            'time_in_hospital', 'num_lab_procedures', 'num_procedures',\n",
    "            'num_medications', 'number_outpatient', 'number_emergency',\n",
    "            'number_inpatient', 'number_diagnoses'\n",
    "        ]\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "        \n",
    "        # Process medication columns - most have 'No', 'Up', 'Down', 'Steady'\n",
    "        medication_cols = [\n",
    "            'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n",
    "            'glimepiride', 'acetohexamide', 'glipizide', 'glyburide',\n",
    "            'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n",
    "            'miglitol', 'troglitazone', 'tolazamide', 'examide',\n",
    "            'citoglipton', 'insulin', 'glyburide-metformin',\n",
    "            'glipizide-metformin', 'glimepiride-pioglitazone',\n",
    "            'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "        ]\n",
    "        \n",
    "        for col in medication_cols:\n",
    "            if col in df_clean.columns:\n",
    "                # Create a binary indicator for medication use (any dosage)\n",
    "                df_clean[f'{col}_used'] = df_clean[col].apply(lambda x: 0 if x == 'No' else 1)\n",
    "                \n",
    "                # Create a numeric dosage change indicator\n",
    "                dosage_map = {'No': 0, 'Steady': 1, 'Up': 2, 'Down': -1}\n",
    "                df_clean[f'{col}_dosage'] = df_clean[col].map(dosage_map)\n",
    "        \n",
    "        # Process diagnosis codes (ICD-9)\n",
    "        diag_cols = ['diag_1', 'diag_2', 'diag_3']\n",
    "        \n",
    "        # Define diagnosis categories based on ICD-9 ranges\n",
    "        def categorize_diagnosis(code):\n",
    "            try:\n",
    "                if pd.isna(code) or code == '?':\n",
    "                    return 'Unknown'\n",
    "                    \n",
    "                # Convert to string and clean\n",
    "                code_str = str(code).strip()\n",
    "                \n",
    "                # Check if it's the ICD-9 code format with V or E prefix\n",
    "                if code_str.startswith('V'):\n",
    "                    return 'V_codes'  # Supplementary classification\n",
    "                elif code_str.startswith('E'):\n",
    "                    return 'E_codes'  # External causes\n",
    "                    \n",
    "                # Try to convert to numeric for range checking\n",
    "                code_num = float(code_str)\n",
    "                \n",
    "                # Categorize based on ICD-9 chapter ranges\n",
    "                if 1 <= code_num < 140:\n",
    "                    return 'Infectious'\n",
    "                elif 140 <= code_num < 240:\n",
    "                    return 'Neoplasms'\n",
    "                elif 240 <= code_num < 280:\n",
    "                    return 'Endocrine'  # Includes diabetes\n",
    "                elif 280 <= code_num < 290:\n",
    "                    return 'Blood'\n",
    "                elif 290 <= code_num < 320:\n",
    "                    return 'Mental'\n",
    "                elif 320 <= code_num < 390:\n",
    "                    return 'Nervous'\n",
    "                elif 390 <= code_num < 460:\n",
    "                    return 'Circulatory'  # Heart disease, hypertension\n",
    "                elif 460 <= code_num < 520:\n",
    "                    return 'Respiratory'\n",
    "                elif 520 <= code_num < 580:\n",
    "                    return 'Digestive'\n",
    "                elif 580 <= code_num < 630:\n",
    "                    return 'Genitourinary'\n",
    "                elif 630 <= code_num < 680:\n",
    "                    return 'Pregnancy'\n",
    "                elif 680 <= code_num < 710:\n",
    "                    return 'Skin'\n",
    "                elif 710 <= code_num < 740:\n",
    "                    return 'Musculoskeletal'\n",
    "                elif 740 <= code_num < 760:\n",
    "                    return 'Congenital'\n",
    "                elif 760 <= code_num < 780:\n",
    "                    return 'Perinatal'\n",
    "                elif 780 <= code_num < 800:\n",
    "                    return 'Symptoms'\n",
    "                elif 800 <= code_num < 1000:\n",
    "                    return 'Injury'\n",
    "                else:\n",
    "                    return 'Other'\n",
    "            except:\n",
    "                return 'Unknown'\n",
    "        \n",
    "        # Apply categorization to diagnosis columns\n",
    "        for col in diag_cols:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[f'{col}_category'] = df_clean[col].apply(categorize_diagnosis)\n",
    "                \n",
    "                # Create diabetes-specific indicator\n",
    "                is_diabetes = df_clean[col].apply(lambda x: \n",
    "                                                 True if not pd.isna(x) and str(x).startswith('250') \n",
    "                                                 else False)\n",
    "                df_clean[f'{col}_is_diabetes'] = is_diabetes.astype(int)\n",
    "                \n",
    "                # Create circulatory system indicator (heart disease, hypertension)\n",
    "                is_circulatory = df_clean[f'{col}_category'] == 'Circulatory'\n",
    "                df_clean[f'{col}_is_circulatory'] = is_circulatory.astype(int)\n",
    "                \n",
    "                # Create respiratory system indicator\n",
    "                is_respiratory = df_clean[f'{col}_category'] == 'Respiratory'\n",
    "                df_clean[f'{col}_is_respiratory'] = is_respiratory.astype(int)\n",
    "        \n",
    "        # Process A1C and glucose results\n",
    "        if 'A1Cresult' in df_clean.columns:\n",
    "            a1c_map = {'>8': 3, '>7': 2, 'Norm': 1, 'None': 0}\n",
    "            df_clean['A1C_value'] = df_clean['A1Cresult'].map(a1c_map)\n",
    "            \n",
    "        if 'max_glu_serum' in df_clean.columns:\n",
    "            glu_map = {'>300': 3, '>200': 2, 'Norm': 1, 'None': 0}\n",
    "            df_clean['glucose_value'] = df_clean['max_glu_serum'].map(glu_map)\n",
    "        \n",
    "        # Process the target variable 'readmitted'\n",
    "        if 'readmitted' in df_clean.columns:\n",
    "            # Create binary target: readmitted within 30 days (1) vs. not (0)\n",
    "            df_clean['readmitted_30d'] = df_clean['readmitted'].apply(\n",
    "                lambda x: 1 if x == '<30' else 0\n",
    "            )\n",
    "            \n",
    "            # Also create a 3-class version: <30 days, >30 days, No readmission\n",
    "            readmit_map = {'<30': 2, '>30': 1, 'NO': 0}\n",
    "            df_clean['readmitted_class'] = df_clean['readmitted'].map(readmit_map)\n",
    "        \n",
    "        # Drop the original ID columns if we've created the mapped versions\n",
    "        if 'admission_type' in df_clean.columns:\n",
    "            df_clean.drop('admission_type_id', axis=1, inplace=True, errors='ignore')\n",
    "            \n",
    "        if 'discharge_disposition' in df_clean.columns:\n",
    "            df_clean.drop('discharge_disposition_id', axis=1, inplace=True, errors='ignore')\n",
    "            \n",
    "        if 'admission_source' in df_clean.columns:\n",
    "            df_clean.drop('admission_source_id', axis=1, inplace=True, errors='ignore')\n",
    "        \n",
    "        # Check for missing values after processing\n",
    "        missing_after = df_clean.isnull().sum()\n",
    "        print(f\"Missing values after processing:\\n{missing_after[missing_after > 0]}\")\n",
    "        \n",
    "        return df_clean\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed during data cleaning: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"Create additional features for modeling\"\"\"\n",
    "    try:\n",
    "        df_featured = df.copy()\n",
    "        \n",
    "        # Age category feature\n",
    "        if 'age' in df_featured.columns:\n",
    "            # Create numeric age from categorical ranges\n",
    "            def age_to_numeric(age_range):\n",
    "                age_map = {\n",
    "                    '[0-10)': 5,\n",
    "                    '[10-20)': 15,\n",
    "                    '[20-30)': 25,\n",
    "                    '[30-40)': 35,\n",
    "                    '[40-50)': 45,\n",
    "                    '[50-60)': 55,\n",
    "                    '[60-70)': 65,\n",
    "                    '[70-80)': 75,\n",
    "                    '[80-90)': 85,\n",
    "                    '[90-100)': 95\n",
    "                }\n",
    "                return age_map.get(age_range, 50)  # Default to 50 if unknown\n",
    "                \n",
    "            df_featured['age_numeric'] = df_featured['age'].apply(age_to_numeric)\n",
    "            \n",
    "            # Create age group categories\n",
    "            df_featured['age_group'] = pd.cut(\n",
    "                df_featured['age_numeric'],\n",
    "                bins=[0, 30, 50, 70, 100],\n",
    "                labels=['Young', 'Middle', 'Senior', 'Elderly']\n",
    "            )\n",
    "        \n",
    "        # Total number of visits feature\n",
    "        visit_cols = ['number_outpatient', 'number_emergency', 'number_inpatient']\n",
    "        if all(col in df_featured.columns for col in visit_cols):\n",
    "            df_featured['total_visits'] = df_featured[visit_cols].sum(axis=1)\n",
    "        \n",
    "        # Medication complexity features\n",
    "        med_used_cols = [col for col in df_featured.columns if col.endswith('_used')]\n",
    "        if med_used_cols:\n",
    "            # Total number of medications used\n",
    "            df_featured['total_meds_used'] = df_featured[med_used_cols].sum(axis=1)\n",
    "            \n",
    "            # Medication diversity ratio (unique meds / total possible meds)\n",
    "            df_featured['med_diversity_ratio'] = df_featured['total_meds_used'] / len(med_used_cols)\n",
    "            \n",
    "        # Diagnosis complexity feature\n",
    "        if 'number_diagnoses' in df_featured.columns:\n",
    "            # Create categories for diagnosis count\n",
    "            df_featured['diagnosis_complexity'] = pd.cut(\n",
    "                df_featured['number_diagnoses'],\n",
    "                bins=[0, 3, 6, 9, 20],\n",
    "                labels=['Low', 'Medium', 'High', 'Very High']\n",
    "            )\n",
    "        \n",
    "        # Hospital stay features\n",
    "        if 'time_in_hospital' in df_featured.columns:\n",
    "            # Create categories for length of stay\n",
    "            df_featured['stay_length_cat'] = pd.cut(\n",
    "                df_featured['time_in_hospital'],\n",
    "                bins=[0, 3, 7, 14, 100],\n",
    "                labels=['Short', 'Medium', 'Long', 'Very Long']\n",
    "            )\n",
    "            \n",
    "        # Lab procedure intensity\n",
    "        if 'num_lab_procedures' in df_featured.columns:\n",
    "            df_featured['lab_intensity'] = pd.cut(\n",
    "                df_featured['num_lab_procedures'],\n",
    "                bins=[0, 25, 50, 75, 1000],\n",
    "                labels=['Low', 'Medium', 'High', 'Very High']\n",
    "            )\n",
    "        \n",
    "        # Feature for patients with diabetes as primary diagnosis\n",
    "        diag_diabetes_cols = [col for col in df_featured.columns if col.endswith('_is_diabetes')]\n",
    "        if diag_diabetes_cols and len(diag_diabetes_cols) >= 1:\n",
    "            df_featured['primary_diabetes'] = df_featured['diag_1_is_diabetes']\n",
    "            \n",
    "            if len(diag_diabetes_cols) >= 3:\n",
    "                # Any diabetes diagnosis\n",
    "                df_featured['any_diabetes_diag'] = (\n",
    "                    df_featured['diag_1_is_diabetes'] | \n",
    "                    df_featured['diag_2_is_diabetes'] | \n",
    "                    df_featured['diag_3_is_diabetes']\n",
    "                ).astype(int)\n",
    "        \n",
    "        # Comorbidity features\n",
    "        # Check for common comorbidities (circulatory, respiratory)\n",
    "        circ_cols = [col for col in df_featured.columns if col.endswith('_is_circulatory')]\n",
    "        resp_cols = [col for col in df_featured.columns if col.endswith('_is_respiratory')]\n",
    "        \n",
    "        if circ_cols:\n",
    "            df_featured['has_circulatory_disease'] = df_featured[circ_cols].max(axis=1)\n",
    "            \n",
    "        if resp_cols:\n",
    "            df_featured['has_respiratory_disease'] = df_featured[resp_cols].max(axis=1)\n",
    "            \n",
    "        if circ_cols and resp_cols:\n",
    "            df_featured['circulatory_respiratory_comorbidity'] = (\n",
    "                (df_featured['has_circulatory_disease'] == 1) & \n",
    "                (df_featured['has_respiratory_disease'] == 1)\n",
    "            ).astype(int)\n",
    "        \n",
    "        # Insulin-related features\n",
    "        if 'insulin_used' in df_featured.columns:\n",
    "            # Combine insulin usage with A1C levels for a risk feature\n",
    "            if 'A1C_value' in df_featured.columns:\n",
    "                df_featured['insulin_with_high_A1C'] = (\n",
    "                    (df_featured['insulin_used'] == 1) & \n",
    "                    (df_featured['A1C_value'] >= 2)  # A1C > 7\n",
    "                ).astype(int)\n",
    "        \n",
    "        # Multiple medication changes feature\n",
    "        dosage_cols = [col for col in df_featured.columns if col.endswith('_dosage')]\n",
    "        if dosage_cols:\n",
    "            # Count medication changes (up or down)\n",
    "            df_featured['med_changes_count'] = df_featured[dosage_cols].apply(\n",
    "                lambda row: sum([1 for val in row if val != 0 and val != 1]), axis=1\n",
    "            )\n",
    "        \n",
    "        print(f\"Feature engineering complete. New shape: {df_featured.shape}\")\n",
    "        return df_featured\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed during feature creation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_modeling_data(df, target_col='readmitted_30d', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare the final dataset for modeling:\n",
    "    - Select relevant features\n",
    "    - Handle remaining missing values\n",
    "    - Convert categorical variables\n",
    "    - Scale numerical features\n",
    "    - Split into train/test sets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        df_model = df.copy()\n",
    "        \n",
    "        # Ensure target column exists\n",
    "        if target_col not in df_model.columns:\n",
    "            print(ValueError(f\"Target column '{target_col}' not found in dataset\"))\n",
    "        \n",
    "        # Define feature groups\n",
    "        \n",
    "        # Important categorical features to encode\n",
    "        cat_features = [\n",
    "            'gender', 'race', 'age', 'admission_type', 'discharge_disposition',\n",
    "            'admission_source', 'medical_specialty', 'diag_1_category',\n",
    "            'diag_2_category', 'diag_3_category', 'A1Cresult', 'max_glu_serum',\n",
    "            'age_group', 'diagnosis_complexity', 'stay_length_cat', 'lab_intensity'\n",
    "        ]\n",
    "        \n",
    "        # Important numerical features to scale\n",
    "        num_features = [\n",
    "            'time_in_hospital', 'num_lab_procedures', 'num_procedures',\n",
    "            'num_medications', 'number_outpatient', 'number_emergency',\n",
    "            'number_inpatient', 'number_diagnoses', 'age_numeric',\n",
    "            'total_visits', 'total_meds_used', 'med_diversity_ratio',\n",
    "            'med_changes_count'\n",
    "        ]\n",
    "        \n",
    "        # Binary features (already encoded)\n",
    "        binary_features = [\n",
    "            'diabetesMed', 'change', 'primary_diabetes', 'any_diabetes_diag',\n",
    "            'has_circulatory_disease', 'has_respiratory_disease',\n",
    "            'circulatory_respiratory_comorbidity', 'insulin_with_high_A1C'\n",
    "        ]\n",
    "        \n",
    "        # Binary medication indicators\n",
    "        med_features = [col for col in df_model.columns if col.endswith('_used')]\n",
    "        \n",
    "        # Filter to include only features that exist in the dataframe\n",
    "        cat_features = [col for col in cat_features if col in df_model.columns]\n",
    "        num_features = [col for col in num_features if col in df_model.columns]\n",
    "        binary_features = [col for col in binary_features if col in df_model.columns]\n",
    "        med_features = [col for col in med_features if col in df_model.columns]\n",
    "        \n",
    "        # Check for duplicate column names in each feature group\n",
    "        all_features = []\n",
    "        for feature_list in [cat_features, num_features, binary_features, med_features]:\n",
    "            # Add only unique columns not already in all_features\n",
    "            for feature in feature_list:\n",
    "                if feature not in all_features:\n",
    "                    all_features.append(feature)\n",
    "        \n",
    "        print(f\"Selected {len(all_features)} features for modeling\")\n",
    "        \n",
    "        # Create a new dataframe with only the selected features\n",
    "        X = df_model[all_features].copy()\n",
    "        y = df_model[target_col].copy()\n",
    "        \n",
    "        # Double check for duplicate columns in X\n",
    "        duplicate_cols = X.columns[X.columns.duplicated()]\n",
    "        if len(duplicate_cols) > 0:\n",
    "            print(f\"Found duplicate columns: {duplicate_cols.tolist()}\")\n",
    "            # Create a new dataframe with unique column names\n",
    "            X_columns = list(X.columns)\n",
    "            unique_columns = []\n",
    "            for i, col in enumerate(X_columns):\n",
    "                if col in unique_columns:\n",
    "                    new_col = f\"{col}_{unique_columns.count(col)}\"\n",
    "                    print(f\"Renaming duplicate column {col} to {new_col}\")\n",
    "                    X_columns[i] = new_col\n",
    "                unique_columns.append(X_columns[i])\n",
    "            X.columns = X_columns\n",
    "        \n",
    "        # Split into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Train set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "        \n",
    "        # Determine which columns are numerical and categorical after potential renaming\n",
    "        num_features = [col for col in X.columns if col in num_features or any(col.startswith(f) and col.endswith('_0') for f in num_features)]\n",
    "        cat_features = [col for col in X.columns if col in cat_features or any(col.startswith(f) and col.endswith('_0') for f in cat_features)]\n",
    "        \n",
    "        # Create preprocessing pipeline\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "        from sklearn.compose import ColumnTransformer\n",
    "        \n",
    "        # Create feature transformation pipelines\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        # Use sparse_output instead of sparse for newer sklearn versions\n",
    "        import sklearn\n",
    "        from packaging import version\n",
    "        \n",
    "        if version.parse(sklearn.__version__) >= version.parse('1.2.0'):\n",
    "            categorical_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "            ])\n",
    "        else:\n",
    "            categorical_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "            ])\n",
    "        \n",
    "        # Check for any remaining string/object columns that might have been missed\n",
    "        object_columns = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "        if object_columns:\n",
    "            print(f\"Found {len(object_columns)} object columns that need encoding: {object_columns}\")\n",
    "            # Add these to cat_features if not already included\n",
    "            for col in object_columns:\n",
    "                if col not in cat_features:\n",
    "                    cat_features.append(col)\n",
    "                    print(f\"Added {col} to categorical features list\")\n",
    "        \n",
    "        # Apply different preprocessing to different feature types\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, num_features),\n",
    "                ('cat', categorical_transformer, cat_features)\n",
    "            ],\n",
    "            remainder='passthrough'  # Keep binary features as is\n",
    "        )\n",
    "        \n",
    "        # Fit the preprocessor to the training data\n",
    "        X_train_processed = preprocessor.fit_transform(X_train)\n",
    "        X_test_processed = preprocessor.transform(X_test)\n",
    "        \n",
    "        # Get feature names for categorical variables after one-hot encoding\n",
    "        cat_feature_names = []\n",
    "        if len(cat_features) > 0:  # Only do this if we have categorical features\n",
    "            for i, feature in enumerate(cat_features):\n",
    "                encoder = preprocessor.transformers_[1][1].named_steps['onehot']\n",
    "                feature_categories = encoder.categories_[i]\n",
    "                cat_feature_names.extend([f\"{feature}_{cat}\" for cat in feature_categories])\n",
    "        \n",
    "        # Get remaining feature names\n",
    "        remaining_features = [col for col in X.columns if col not in num_features and col not in cat_features]\n",
    "        \n",
    "        # Combine all feature names\n",
    "        feature_names = num_features + cat_feature_names + remaining_features\n",
    "        \n",
    "        # Save the model-ready data\n",
    "        processed_data = {\n",
    "            'X_train': X_train_processed,\n",
    "            'X_test': X_test_processed,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test,\n",
    "            'feature_names': feature_names,\n",
    "            'preprocessor': preprocessor\n",
    "        }\n",
    "        \n",
    "        # Save preprocessed data for modeling\n",
    "        import joblib\n",
    "        joblib.dump(processed_data, PROCESSED_DATA_DIR / \"model_ready_data.pkl\")\n",
    "        print(f\"Model-ready data saved to {PROCESSED_DATA_DIR / 'model_ready_data.pkl'}\")\n",
    "        \n",
    "        # Also save the pre-processed dataframe for reference\n",
    "        df_model.to_csv(PROCESSED_DATA_PATH, index=False)\n",
    "        print(f\"Processed dataframe saved to {PROCESSED_DATA_PATH}\")\n",
    "        \n",
    "        return processed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed during modeling data preparation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Performing Preprocessing of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = load_raw_data()\n",
    "mappings = load_mappings()\n",
    "\n",
    "# Clean data\n",
    "df_clean = clean_data(df, mappings)\n",
    "\n",
    "# Create features\n",
    "df_featured = create_features(df_clean)\n",
    "\n",
    "# Prepare for modeling\n",
    "processed_data = prepare_modeling_data(df_featured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
